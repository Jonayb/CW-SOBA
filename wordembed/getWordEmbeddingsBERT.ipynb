{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getWordEmbeddingsBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P29NvvnypuRx"
      },
      "source": [
        "# CREATING WORD EMBEDDINGS FOR ONTOLOGY USE\n",
        "# --------------------------------------------------\n",
        "#\n",
        "# Adapted by Jonathan IJbema\n",
        "#\n",
        "#\n",
        "# Transform reviews into wordembeddings and save them in a json-style text file.\n",
        "# 1. Choose between models: (1) BERT, (2) RoBERTa\n",
        "# 2. Load dataset in special format. Reviews must be separated by ,|,\n",
        "# 3. Remove negations from dataset\n",
        "# 4. Create word embeddings\n",
        "# 5. Save word embeddings to json-style text file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOQlgS8BeV9C"
      },
      "source": [
        "#@title Model choice\n",
        "#@markdown Choose which model to use for tokenizing and embedding.\n",
        "\n",
        "modelChoice = 'RoBERTa' # @param [\"BERT\", \"RoBERTa\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxWKl9MWVT4M",
        "outputId": "ce1fde8a-4f3e-4916-ee5e-d55c5092d8cf"
      },
      "source": [
        "# Install and import libraries\n",
        "#!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "if modelChoice == 'BERT':\n",
        "  mc = 1\n",
        "else:\n",
        "  mc = 2\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "if mc == 1:\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  print(\"Model choice: BERT. Important libraries and models imported.\")\n",
        "elif mc == 2:\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  print(\"Model choice: RoBERTa. Important libraries and models imported.\")\n",
        "else:\n",
        "  raise Exception(\"No tokenizer chosen. Choose between 1 and 2.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model choice: RoBERTa. Important libraries and models imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPXaBY07VT4P",
        "outputId": "4eefdb9c-c28b-4559-d6bc-43e196b94030"
      },
      "source": [
        "# Load data for creating word embeddings\n",
        "\n",
        "#@title File path\n",
        "#@markdown Choose file path for dataset.\n",
        "\n",
        "file_path = 'data/restData5k.txt' #@param {type:\"string\"}\n",
        "\n",
        "i=0\n",
        "reviews=[]\n",
        "file = open(file_path, 'r', encoding=\"utf-8\")\n",
        "review = file.read()\n",
        "review = review.replace(\"\\n\", ' ')\n",
        "\n",
        "#Reviews are split on ,|,\n",
        "wrong_review = review.split(\",|,\") \n",
        "for w in wrong_review:\n",
        "    i += 1\n",
        "    reviews.append(w)\n",
        "reviews = reviews[0:2000] # Choose subset from reviews\n",
        "#reviews = sample(reviews, 2000) # Choose random subset from reviews\n",
        "print(\"Number of reviews : \", len(reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews :  2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzqjTpMBVT4Q",
        "outputId": "9aef816e-c2ef-4c2c-fcd3-499e3a499cb1"
      },
      "source": [
        "# Code for removing negation words in same sentence\n",
        "i = 0\n",
        "review = 0\n",
        "sentC=0\n",
        "reviewsNew = []\n",
        "neg = \" not \"\n",
        "neg2 = \" nothing \"\n",
        "neg3 = \"never \"\n",
        "neg4 = \" didn\\'t\"\n",
        "neg5 = \" wouldn\\'t\"\n",
        "neg6 = \" don\\'t\"\n",
        "neg7 = \" can\\'t\"\n",
        "neg8 = \" doesn\\'t\"\n",
        "neg9 = \" coudn't\"\n",
        "case=0\n",
        "for z in reviews:\n",
        "    sent2New = []\n",
        "    sentNew= \"\"\n",
        "    sent = z.split('.')\n",
        "    case = 0\n",
        "    for j in sent:\n",
        "        sent2 = j.split('!')\n",
        "        for l in sent2:\n",
        "            sentC+=1\n",
        "            if (neg in l) or (neg2 in l) or (neg3 in l) or (neg4 in l) or (neg5 in l) or (neg6 in l) or (neg7 in l) or (neg8 in l) or (neg9 in l):\n",
        "                i += 1\n",
        "                sent2.remove(l)\n",
        "                if case == 0:\n",
        "                    review += 1\n",
        "                    case = 1\n",
        "        sent2New.append(\"!\".join(sent2))\n",
        "    #print(sent2New)\n",
        "    sentNew = (\".\".join(sent2New))\n",
        "    reviewsNew.append(str(sentNew))\n",
        "\n",
        "print(\"Number of sentences with negation word:\", i)\n",
        "print(\"Number of reviews with these negation word:\" ,review)\n",
        "print(\"Number of sentences in text: \", sentC)\n",
        "print(\"Number of sentences in text after removing sentences: \", sentC - i )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences with negation word: 885\n",
            "Number of reviews with these negation word: 687\n",
            "Number of sentences in text:  12194\n",
            "Number of sentences in text after removing sentences:  11309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzeRw2jFVT4Q",
        "outputId": "658b35ff-0e42-4c1b-de08-6fa8e500671a"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "\n",
        "#@title Model configuration\n",
        "#@markdown Choose model configuration.\n",
        "finetuned = True #@param {type:\"boolean\"}\n",
        "posttrained = False #@param {type:\"boolean\"}\n",
        "\n",
        "if mc == 1:\n",
        "  if posttrained:\n",
        "    if finetuned:\n",
        "      model = BertModel.from_pretrained('models/finetunedPT', output_hidden_states = True,)\n",
        "    else:\n",
        "      model = BertModel.from_pretrained('models/posttrained', output_hidden_states = True,)\n",
        "  else:\n",
        "    if finetuned:\n",
        "      model = BertModel.from_pretrained('models/finetuned', output_hidden_states = True,)\n",
        "    else:\n",
        "      model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
        "  print(\"BERT model is downloaded.\")\n",
        "elif mc == 2:\n",
        "  if finetuned:\n",
        "    model = RobertaModel.from_pretrained('models/Roberta_finetuned', output_hidden_states = True,)\n",
        "  else:\n",
        "    model = RobertaModel.from_pretrained('roberta-base', output_hidden_states = True,)\n",
        "  print(\"RoBERTa model is downloaded.\")\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at DCWEB/Roberta_finetuned were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at DCWEB/Roberta_finetuned and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RoBERTa model is downloaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uut0JqmnVT4R",
        "outputId": "677b076b-c695-449b-c249-2ab29a0e234d"
      },
      "source": [
        "#Create word embeddings\n",
        "start = time.time()\n",
        "intermediate_time = start\n",
        "bert_vectors = []\n",
        "review_counter = 0\n",
        "j = 1\n",
        "#loops over the reviews\n",
        "for rev in reviews:\n",
        "    rev = rev.lower().replace('\\n', '').replace('\\r', '').strip()\n",
        "    update = 20\n",
        "    if review_counter % update == 0:\n",
        "      start_time = intermediate_time\n",
        "      intermediate_time = time.time()\n",
        "      embedding_time = (intermediate_time - start_time)\n",
        "      if review_counter < 40:\n",
        "        average = embedding_time\n",
        "      else:\n",
        "        average = (average*(review_counter/20 - 1) + embedding_time) / (review_counter/20)\n",
        "      ETA = average*(len(reviews)/update-review_counter/update)\n",
        "      print(\"Estimated remaining time: \", str(round(ETA/60, 1)), \" minutes.\")\n",
        "      print(\"Embedding review: \" + str(review_counter))\n",
        "\n",
        "    result = []\n",
        "    tokenized_text = tokenizer.tokenize(rev)\n",
        "    #change to 512 or shorter\n",
        "    if len(tokenized_text)>=512:\n",
        "        del tokenized_text[512:len(tokenized_text)]\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "    if len(tokenized_text) == 0 :\n",
        "        print(\"Word with 0 length.\")\n",
        "        continue\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        hidden_states = outputs.hidden_states\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)\n",
        "    token_vecs_sum = []\n",
        "    for token in token_embeddings:\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "    token_vecs = hidden_states[-2][0]\n",
        "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "    #get all results for one review and display\n",
        "    # replace with whole vector!\n",
        "    i=0\n",
        "    veccieTemp = []\n",
        "    while i < len(tokenized_text):\n",
        "        veccie = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
        "        if mc == 1:\n",
        "          if not tokenized_text[i].isalpha():  # remove all non alphabetic characters\n",
        "            i += 1\n",
        "            continue\n",
        "          result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
        "        else:\n",
        "          text = tokenizer.decode(indexed_tokens[i]).strip()\n",
        "          if text == 'amb':\n",
        "            veccieTemp = veccie #ambience is split up by RoBERTa. This makes sure ambience will be added to the word vectors\n",
        "            i += 1\n",
        "            continue\n",
        "          if text == 'ience':\n",
        "            veccie = np.array(veccie)\n",
        "            veccieTemp = np.array(veccieTemp)\n",
        "            if (veccieTemp.shape[0] > 0):\n",
        "              veccie = np.vstack((veccie, veccieTemp))\n",
        "            else:\n",
        "              i += 1\n",
        "              continue\n",
        "            veccie = np.mean(veccie, axis=0)\n",
        "            result.append(['ambience',indexed_tokens[i],segments_ids[i], [round(num, 4) for num in veccie], review_counter])\n",
        "            i += 1\n",
        "            continue\n",
        "          if not text.isalpha():  # remove all non alphabetic characters\n",
        "            i += 1\n",
        "            continue\n",
        "          result.append([text,indexed_tokens[i],segments_ids[i], veccie, review_counter])\n",
        "        i += 1\n",
        "    \n",
        "    bert_vectors.append(result)\n",
        "\n",
        "    del result\n",
        "    review_counter+=1\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time: \", end-start, \"s\")\n",
        "print(\"Reviews are tokenized and put into vectors!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated remaining time:  0.0  minutes.\n",
            "Embedding review: 0\n",
            "Estimated remaining time:  4.8  minutes.\n",
            "Embedding review: 20\n",
            "Estimated remaining time:  4.3  minutes.\n",
            "Embedding review: 40\n",
            "Estimated remaining time:  4.5  minutes.\n",
            "Embedding review: 60\n",
            "Estimated remaining time:  4.3  minutes.\n",
            "Embedding review: 80\n",
            "Estimated remaining time:  4.3  minutes.\n",
            "Embedding review: 100\n",
            "Estimated remaining time:  4.3  minutes.\n",
            "Embedding review: 120\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 140\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 160\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 180\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 200\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 220\n",
            "Estimated remaining time:  4.2  minutes.\n",
            "Embedding review: 240\n",
            "Estimated remaining time:  4.1  minutes.\n",
            "Embedding review: 260\n",
            "Estimated remaining time:  4.1  minutes.\n",
            "Embedding review: 280\n",
            "Estimated remaining time:  4.0  minutes.\n",
            "Embedding review: 300\n",
            "Estimated remaining time:  4.0  minutes.\n",
            "Embedding review: 320\n",
            "Estimated remaining time:  4.0  minutes.\n",
            "Embedding review: 340\n",
            "Estimated remaining time:  4.0  minutes.\n",
            "Embedding review: 360\n",
            "Estimated remaining time:  4.0  minutes.\n",
            "Embedding review: 380\n",
            "Estimated remaining time:  3.9  minutes.\n",
            "Embedding review: 400\n",
            "Estimated remaining time:  3.9  minutes.\n",
            "Embedding review: 420\n",
            "Estimated remaining time:  3.9  minutes.\n",
            "Embedding review: 440\n",
            "Estimated remaining time:  3.8  minutes.\n",
            "Embedding review: 460\n",
            "Estimated remaining time:  3.8  minutes.\n",
            "Embedding review: 480\n",
            "Estimated remaining time:  3.8  minutes.\n",
            "Embedding review: 500\n",
            "Estimated remaining time:  3.7  minutes.\n",
            "Embedding review: 520\n",
            "Estimated remaining time:  3.7  minutes.\n",
            "Embedding review: 540\n",
            "Estimated remaining time:  3.7  minutes.\n",
            "Embedding review: 560\n",
            "Estimated remaining time:  3.6  minutes.\n",
            "Embedding review: 580\n",
            "Estimated remaining time:  3.6  minutes.\n",
            "Embedding review: 600\n",
            "Estimated remaining time:  3.5  minutes.\n",
            "Embedding review: 620\n",
            "Estimated remaining time:  3.5  minutes.\n",
            "Embedding review: 640\n",
            "Estimated remaining time:  3.4  minutes.\n",
            "Embedding review: 660\n",
            "Estimated remaining time:  3.4  minutes.\n",
            "Embedding review: 680\n",
            "Estimated remaining time:  3.3  minutes.\n",
            "Embedding review: 700\n",
            "Estimated remaining time:  3.3  minutes.\n",
            "Embedding review: 720\n",
            "Estimated remaining time:  3.2  minutes.\n",
            "Embedding review: 740\n",
            "Estimated remaining time:  3.2  minutes.\n",
            "Embedding review: 760\n",
            "Estimated remaining time:  3.1  minutes.\n",
            "Embedding review: 780\n",
            "Estimated remaining time:  3.1  minutes.\n",
            "Embedding review: 800\n",
            "Estimated remaining time:  3.0  minutes.\n",
            "Embedding review: 820\n",
            "Estimated remaining time:  3.0  minutes.\n",
            "Embedding review: 840\n",
            "Estimated remaining time:  2.9  minutes.\n",
            "Embedding review: 860\n",
            "Estimated remaining time:  2.9  minutes.\n",
            "Embedding review: 880\n",
            "Estimated remaining time:  2.8  minutes.\n",
            "Embedding review: 900\n",
            "Estimated remaining time:  2.8  minutes.\n",
            "Embedding review: 920\n",
            "Estimated remaining time:  2.8  minutes.\n",
            "Embedding review: 940\n",
            "Estimated remaining time:  2.7  minutes.\n",
            "Embedding review: 960\n",
            "Estimated remaining time:  2.6  minutes.\n",
            "Embedding review: 980\n",
            "Estimated remaining time:  2.6  minutes.\n",
            "Embedding review: 1000\n",
            "Estimated remaining time:  2.5  minutes.\n",
            "Embedding review: 1020\n",
            "Estimated remaining time:  2.5  minutes.\n",
            "Embedding review: 1040\n",
            "Estimated remaining time:  2.4  minutes.\n",
            "Embedding review: 1060\n",
            "Estimated remaining time:  2.4  minutes.\n",
            "Embedding review: 1080\n",
            "Estimated remaining time:  2.3  minutes.\n",
            "Embedding review: 1100\n",
            "Estimated remaining time:  2.3  minutes.\n",
            "Embedding review: 1120\n",
            "Estimated remaining time:  2.2  minutes.\n",
            "Embedding review: 1140\n",
            "Estimated remaining time:  2.2  minutes.\n",
            "Embedding review: 1160\n",
            "Estimated remaining time:  2.1  minutes.\n",
            "Embedding review: 1180\n",
            "Estimated remaining time:  2.1  minutes.\n",
            "Embedding review: 1200\n",
            "Estimated remaining time:  2.0  minutes.\n",
            "Embedding review: 1220\n",
            "Estimated remaining time:  2.0  minutes.\n",
            "Embedding review: 1240\n",
            "Estimated remaining time:  1.9  minutes.\n",
            "Embedding review: 1260\n",
            "Estimated remaining time:  1.9  minutes.\n",
            "Embedding review: 1280\n",
            "Estimated remaining time:  1.8  minutes.\n",
            "Embedding review: 1300\n",
            "Estimated remaining time:  1.8  minutes.\n",
            "Embedding review: 1320\n",
            "Estimated remaining time:  1.7  minutes.\n",
            "Embedding review: 1340\n",
            "Estimated remaining time:  1.7  minutes.\n",
            "Embedding review: 1360\n",
            "Estimated remaining time:  1.6  minutes.\n",
            "Embedding review: 1380\n",
            "Estimated remaining time:  1.6  minutes.\n",
            "Embedding review: 1400\n",
            "Estimated remaining time:  1.5  minutes.\n",
            "Embedding review: 1420\n",
            "Estimated remaining time:  1.5  minutes.\n",
            "Embedding review: 1440\n",
            "Estimated remaining time:  1.4  minutes.\n",
            "Embedding review: 1460\n",
            "Estimated remaining time:  1.4  minutes.\n",
            "Embedding review: 1480\n",
            "Estimated remaining time:  1.3  minutes.\n",
            "Embedding review: 1500\n",
            "Estimated remaining time:  1.3  minutes.\n",
            "Embedding review: 1520\n",
            "Estimated remaining time:  1.2  minutes.\n",
            "Embedding review: 1540\n",
            "Estimated remaining time:  1.2  minutes.\n",
            "Embedding review: 1560\n",
            "Estimated remaining time:  1.1  minutes.\n",
            "Embedding review: 1580\n",
            "Estimated remaining time:  1.1  minutes.\n",
            "Embedding review: 1600\n",
            "Estimated remaining time:  1.0  minutes.\n",
            "Embedding review: 1620\n",
            "Estimated remaining time:  1.0  minutes.\n",
            "Embedding review: 1640\n",
            "Estimated remaining time:  0.9  minutes.\n",
            "Embedding review: 1660\n",
            "Estimated remaining time:  0.9  minutes.\n",
            "Embedding review: 1680\n",
            "Estimated remaining time:  0.8  minutes.\n",
            "Embedding review: 1700\n",
            "Estimated remaining time:  0.8  minutes.\n",
            "Embedding review: 1720\n",
            "Estimated remaining time:  0.7  minutes.\n",
            "Embedding review: 1740\n",
            "Estimated remaining time:  0.6  minutes.\n",
            "Embedding review: 1760\n",
            "Estimated remaining time:  0.6  minutes.\n",
            "Embedding review: 1780\n",
            "Estimated remaining time:  0.5  minutes.\n",
            "Embedding review: 1800\n",
            "Estimated remaining time:  0.5  minutes.\n",
            "Embedding review: 1820\n",
            "Estimated remaining time:  0.4  minutes.\n",
            "Embedding review: 1840\n",
            "Estimated remaining time:  0.4  minutes.\n",
            "Embedding review: 1860\n",
            "Estimated remaining time:  0.3  minutes.\n",
            "Embedding review: 1880\n",
            "Estimated remaining time:  0.3  minutes.\n",
            "Embedding review: 1900\n",
            "Estimated remaining time:  0.2  minutes.\n",
            "Embedding review: 1920\n",
            "Estimated remaining time:  0.2  minutes.\n",
            "Embedding review: 1940\n",
            "Estimated remaining time:  0.1  minutes.\n",
            "Embedding review: 1960\n",
            "Estimated remaining time:  0.1  minutes.\n",
            "Embedding review: 1980\n",
            "Time:  323.4553027153015 s\n",
            "Reviews are tokenized and put into vectors!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCBydAYEVT4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d41764-2895-4c09-c116-240921ae9dcb"
      },
      "source": [
        "#Create good format for ontology\n",
        "#Change list1 to the location of bert vectors\n",
        "start = time.time()\n",
        "list1 = bert_vectors\n",
        "words = {}\n",
        "j=1\n",
        "counterRev = 0\n",
        "for rev in list1:\n",
        "    counterRev += 1\n",
        "    for word in rev:\n",
        "        string1 = str(word[0])\n",
        "        words[j] = {'word': string1,\n",
        "                    'vector': word[3]\n",
        "                    ,'sentence id': word[4]\n",
        "                   }\n",
        "        j = j+1\n",
        "        \n",
        "print('Number of reviews done: ', counterRev, ', with ', j, ' words in total.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews done:  2000 , with  107045  words in total.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclC9GU5VT4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492de8d4-8a8b-4ba3-904a-e6e5e5e8f3e5"
      },
      "source": [
        "#Save file\n",
        "\n",
        "#@title Output file\n",
        "#@markdown Choose output file name.\n",
        "\n",
        "file_name = \"RobertaFT.txt\" #@param {type:\"string\"}\n",
        "\n",
        "import json\n",
        "start = time.time()\n",
        "with open(file_name, 'w') as outfile:\n",
        "    json.dump(words, outfile)\n",
        "end = time.time()\n",
        "print(\"Time: \", end-start, \"s\")\n",
        "print(\"Vectors are saved in \" + file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  170.22080278396606 s\n",
            "Vectors are saved in RobertaFT.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufKy0nLGVT4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d93f96-57af-40e6-b401-a84111b98df5"
      },
      "source": [
        "# Remove everything from memory\n",
        "import gc\n",
        "del words\n",
        "del bert_vectors\n",
        "del list1\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getWordEmbeddingsBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P29NvvnypuRx"
      },
      "source": [
        "# CREATING WORD EMBEDDINGS FOR ONTOLOGY USE\n",
        "# --------------------------------------------------\n",
        "#\n",
        "# Original code from https://github.com/MarkRademaker/DCWEB-SOBA\n",
        "#\n",
        "# Adapted by Jonathan IJbema\n",
        "#\n",
        "#\n",
        "# Transform reviews into wordembeddings and save them in a json-style text file.\n",
        "# 1. Choose between models: (1) BERT, (2) RoBERTa\n",
        "# 2. Load dataset in special format. Reviews must be separated by ,|,\n",
        "# 3. Remove negations from dataset\n",
        "# 4. Create word embeddings\n",
        "# 5. Save word embeddings to json-style text file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOQlgS8BeV9C"
      },
      "source": [
        "#@title Model choice\n",
        "#@markdown Choose which model to use for tokenizing and embedding.\n",
        "\n",
        "modelChoice = 'RoBERTa' # @param [\"BERT\", \"RoBERTa\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxWKl9MWVT4M",
        "outputId": "9d63256d-b1a6-4b8b-f7be-b3ce8026f70f"
      },
      "source": [
        "# Install and import libraries\n",
        "#!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "if modelChoice == 'BERT':\n",
        "  mc = 1\n",
        "else:\n",
        "  mc = 2\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "if mc == 1:\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "  print(\"Model choice: BERT. Important libraries and models imported.\")\n",
        "elif mc == 2:\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "  print(\"Model choice: RoBERTa. Important libraries and models imported.\")\n",
        "else:\n",
        "  raise Exception(\"No tokenizer chosen. Choose between 1 and 2.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model choice: RoBERTa. Important libraries and models imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPXaBY07VT4P",
        "outputId": "e1cb62da-ed71-4a10-f70f-2ee1a5121dce"
      },
      "source": [
        "# Load data for creating word embeddings\n",
        "\n",
        "#@title File path\n",
        "#@markdown Choose file path for dataset.\n",
        "\n",
        "file_path = 'data/turkey.txt' #@param {type:\"string\"}\n",
        "\n",
        "i=0\n",
        "reviews=[]\n",
        "file = open(file_path, 'r', encoding=\"utf-8\")\n",
        "review = file.read()\n",
        "review = review.replace(\"\\n\", ' ')\n",
        "\n",
        "#Reviews are split on ,|,\n",
        "wrong_review = review.split(\",|,\") \n",
        "for w in wrong_review:\n",
        "    i += 1\n",
        "    reviews.append(w)\n",
        "reviews = reviews[0:2000] # Choose subset from reviews\n",
        "#reviews = sample(reviews, 2000) # Choose random subset from reviews\n",
        "print(\"Number of reviews : \", len(reviews))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews :  14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzqjTpMBVT4Q",
        "outputId": "2cd8f2b2-6c9c-435e-8862-a4c69570d3e1"
      },
      "source": [
        "# Code for removing negation words in same sentence\n",
        "i = 0\n",
        "review = 0\n",
        "sentC=0\n",
        "reviewsNew = []\n",
        "neg = \" not \"\n",
        "neg2 = \" nothing \"\n",
        "neg3 = \"never \"\n",
        "neg4 = \" didn\\'t\"\n",
        "neg5 = \" wouldn\\'t\"\n",
        "neg6 = \" don\\'t\"\n",
        "neg7 = \" can\\'t\"\n",
        "neg8 = \" doesn\\'t\"\n",
        "neg9 = \" coudn't\"\n",
        "case=0\n",
        "for z in reviews:\n",
        "    sent2New = []\n",
        "    sentNew= \"\"\n",
        "    sent = z.split('.')\n",
        "    case = 0\n",
        "    for j in sent:\n",
        "        sent2 = j.split('!')\n",
        "        for l in sent2:\n",
        "            sentC+=1\n",
        "            if (neg in l) or (neg2 in l) or (neg3 in l) or (neg4 in l) or (neg5 in l) or (neg6 in l) or (neg7 in l) or (neg8 in l) or (neg9 in l):\n",
        "                i += 1\n",
        "                sent2.remove(l)\n",
        "                if case == 0:\n",
        "                    review += 1\n",
        "                    case = 1\n",
        "        sent2New.append(\"!\".join(sent2))\n",
        "    #print(sent2New)\n",
        "    sentNew = (\".\".join(sent2New))\n",
        "    reviewsNew.append(str(sentNew))\n",
        "\n",
        "print(\"Number of sentences with negation word:\", i)\n",
        "print(\"Number of reviews with these negation word:\" ,review)\n",
        "print(\"Number of sentences in text: \", sentC)\n",
        "print(\"Number of sentences in text after removing sentences: \", sentC - i )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences with negation word: 0\n",
            "Number of reviews with these negation word: 0\n",
            "Number of sentences in text:  14\n",
            "Number of sentences in text after removing sentences:  14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzeRw2jFVT4Q",
        "outputId": "4a5990fc-61a6-4f3d-b8b5-ec977f5243cc"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "\n",
        "#@title Model configuration\n",
        "#@markdown Choose model configuration.\n",
        "finetuned = False #@param {type:\"boolean\"}\n",
        "posttrained = False #@param {type:\"boolean\"}\n",
        "\n",
        "if mc == 1:\n",
        "  if posttrained:\n",
        "    if finetuned:\n",
        "      model = BertModel.from_pretrained('models/finetunedPT', output_hidden_states = True,)\n",
        "    else:\n",
        "      model = BertModel.from_pretrained('models/posttrained', output_hidden_states = True,)\n",
        "  else:\n",
        "    if finetuned:\n",
        "      model = BertModel.from_pretrained('models/finetuned', output_hidden_states = True,)\n",
        "    else:\n",
        "      model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
        "  print(\"BERT model is downloaded.\")\n",
        "elif mc == 2:\n",
        "  if finetuned:\n",
        "    model = RobertaModel.from_pretrained('models/Roberta_finetuned', output_hidden_states = True,)\n",
        "  else:\n",
        "    model = RobertaModel.from_pretrained('roberta-base', output_hidden_states = True,)\n",
        "  print(\"RoBERTa model is downloaded.\")\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RoBERTa model is downloaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uut0JqmnVT4R",
        "outputId": "dbdc3867-15fd-44fd-f331-b0eaba231a44"
      },
      "source": [
        "#Create word embeddings\n",
        "start = time.time()\n",
        "intermediate_time = start\n",
        "bert_vectors = []\n",
        "review_counter = 0\n",
        "j = 1\n",
        "#loops over the reviews\n",
        "for rev in reviews:\n",
        "    rev = rev.replace('\\n', '').replace('\\r', '').strip()\n",
        "    update = 20\n",
        "    if review_counter % update == 0:\n",
        "      start_time = intermediate_time\n",
        "      intermediate_time = time.time()\n",
        "      embedding_time = (intermediate_time - start_time)\n",
        "      if review_counter < 40:\n",
        "        average = embedding_time\n",
        "      else:\n",
        "        average = (average*(review_counter/20 - 1) + embedding_time) / (review_counter/20)\n",
        "      ETA = average*(len(reviews)/update-review_counter/update)\n",
        "      print(\"Estimated remaining time: \", str(round(ETA/60, 1)), \" minutes.\")\n",
        "      print(\"Embedding review: \" + str(review_counter))\n",
        "\n",
        "    result = []\n",
        "    tokenized_text = tokenizer.tokenize(rev)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        hidden_states = outputs.hidden_states\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)\n",
        "    token_vecs_sum = []\n",
        "    for token in token_embeddings:\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "\n",
        "    i=0\n",
        "    vecTemp = []\n",
        "    while i < len(tokenized_text):\n",
        "        embed_vector = [round(vec,4) for vec in token_vecs_sum[i].tolist()]\n",
        "        if mc == 1:  # BERT embedding\n",
        "          if not tokenized_text[i].isalpha():  # remove all non alphabetic characters\n",
        "            i += 1\n",
        "            continue\n",
        "          result.append([tokenized_text[i],indexed_tokens[i],segments_ids[i], embed_vector, review_counter])\n",
        "        else:  # RoBERTa embedding\n",
        "          text = tokenizer.decode(indexed_tokens[i]).strip()\n",
        "          if text == 'amb':\n",
        "            vecTemp = embed_vector #ambience is split up by RoBERTa. This makes sure ambience will be added to the word vectors\n",
        "            i += 1\n",
        "            continue\n",
        "          if text == 'ience':\n",
        "            embed_vector = np.array(embed_vector)\n",
        "            vecTemp = np.array(vecTemp)\n",
        "            if (vecTemp.shape[0] > 0):\n",
        "              embed_vector = np.vstack((embed_vector, vecTemp))\n",
        "            else:\n",
        "              i += 1\n",
        "              continue\n",
        "            embed_vector = np.mean(embed_vector, axis=0)\n",
        "            result.append(['ambience',indexed_tokens[i],segments_ids[i], [round(num, 4) for num in embed_vector], review_counter])\n",
        "            i += 1\n",
        "            continue\n",
        "          if not text.isalpha():  # remove all non alphabetic characters\n",
        "            i += 1\n",
        "            continue\n",
        "          result.append([text.lower(),indexed_tokens[i],segments_ids[i], embed_vector, review_counter])\n",
        "        i += 1\n",
        "    \n",
        "    bert_vectors.append(result)\n",
        "\n",
        "    del result\n",
        "    review_counter+=1\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time: \", end-start, \"s\")\n",
        "print(\"Reviews are tokenized and put into vectors!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated remaining time:  0.0  minutes.\n",
            "Embedding review: 0\n",
            "Time:  0.5919151306152344 s\n",
            "Reviews are tokenized and put into vectors!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCBydAYEVT4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e3ec0d-734a-40db-fbc9-7445c7124993"
      },
      "source": [
        "#Create good format for ontology\n",
        "#Change list1 to the location of bert vectors\n",
        "start = time.time()\n",
        "list1 = bert_vectors\n",
        "words = {}\n",
        "j=1\n",
        "counterRev = 0\n",
        "for rev in list1:\n",
        "    counterRev += 1\n",
        "    for word in rev:\n",
        "        string1 = str(word[0])\n",
        "        words[j] = {'word': string1,\n",
        "                    'vector': word[3]\n",
        "                    ,'sentence id': word[4]\n",
        "                   }\n",
        "        j = j+1\n",
        "        \n",
        "print('Number of reviews done: ', counterRev, ', with ', j, ' words in total.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews done:  14 , with  120  words in total.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nclC9GU5VT4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2276d9c6-9eff-4889-e503-af8d09daf677"
      },
      "source": [
        "#Save file\n",
        "\n",
        "#@title Output file\n",
        "#@markdown Choose output file name.\n",
        "\n",
        "file_name = \"output.txt\" #@param {type:\"string\"}\n",
        "\n",
        "import json\n",
        "start = time.time()\n",
        "with open(file_name, 'w') as outfile:\n",
        "    json.dump(words, outfile)\n",
        "end = time.time()\n",
        "print(\"Time: \", end-start, \"s\")\n",
        "print(\"Vectors are saved in \" + file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time:  0.24334931373596191 s\n",
            "Vectors are saved in output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufKy0nLGVT4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9e7fe5-fef2-40c3-db17-7f36345d3358"
      },
      "source": [
        "# Remove everything from memory\n",
        "import gc\n",
        "del words\n",
        "del bert_vectors\n",
        "del list1\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}